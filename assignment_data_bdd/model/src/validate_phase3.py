"""
Phase 3 Validation Script

This script validates all Phase 3 evaluation and visualization components:
1. Metrics computation (metrics.py)
2. Evaluation pipeline (evaluate.py)
3. Quantitative visualizations (visualize_metrics.py)
4. Qualitative visualizations (visualize_predictions.py)
5. Failure analysis (failure_analysis.py)

Author: Bosch Assignment - Phase 3 Validation
Date: November 2025
"""

import sys
import os
from pathlib import Path
import importlib
import traceback

# Add parent directory to path
script_dir = Path(__file__).resolve().parent
sys.path.insert(0, str(script_dir))

print("=" * 80)
print("Phase 3 Validation Suite".center(80))
print("=" * 80)
print()


def test_import(module_name: str, description: str) -> bool:
    """Test if a module can be imported."""
    print(f"Testing: {description}")
    print(f"  Module: {module_name}")
    
    try:
        module = importlib.import_module(module_name)
        print(f"  ‚úì Import successful")
        return True
    except Exception as e:
        print(f"  ‚úó Import failed: {e}")
        traceback.print_exc()
        return False


def test_function_exists(module_name: str, function_name: str, description: str) -> bool:
    """Test if a function exists in a module."""
    print(f"Testing: {description}")
    print(f"  Function: {module_name}.{function_name}")
    
    try:
        module = importlib.import_module(module_name)
        if hasattr(module, function_name):
            print(f"  ‚úì Function exists")
            return True
        else:
            print(f"  ‚úó Function not found")
            return False
    except Exception as e:
        print(f"  ‚úó Error: {e}")
        return False


def test_class_exists(module_name: str, class_name: str, description: str) -> bool:
    """Test if a class exists in a module."""
    print(f"Testing: {description}")
    print(f"  Class: {module_name}.{class_name}")
    
    try:
        module = importlib.import_module(module_name)
        if hasattr(module, class_name):
            cls = getattr(module, class_name)
            print(f"  ‚úì Class exists")
            return True
        else:
            print(f"  ‚úó Class not found")
            return False
    except Exception as e:
        print(f"  ‚úó Error: {e}")
        return False


def test_metrics_module():
    """Test metrics.py module."""
    print("\n" + "=" * 80)
    print("1. TESTING METRICS MODULE (metrics.py)")
    print("=" * 80 + "\n")
    
    results = []
    
    # Test imports
    results.append(test_import('metrics', 'Import metrics module'))
    print()
    
    # Test key functions
    results.append(test_function_exists('metrics', 'compute_iou', 'IoU computation function'))
    print()
    results.append(test_function_exists('metrics', 'compute_ap', 'Average Precision function'))
    print()
    results.append(test_function_exists('metrics', 'compute_map', 'Mean Average Precision function'))
    print()
    results.append(test_function_exists('metrics', 'compute_precision_recall_f1', 'Precision/Recall/F1 function'))
    print()
    results.append(test_function_exists('metrics', 'compute_confusion_matrix', 'Confusion matrix function'))
    print()
    
    # Test actual computation
    print("Testing: Metrics computation with dummy data")
    try:
        import numpy as np
        from metrics import compute_iou, compute_map
        
        # Test IoU
        box1 = np.array([0, 0, 10, 10])
        box2 = np.array([5, 5, 15, 15])
        iou = compute_iou(box1, box2)
        print(f"  IoU test result: {iou:.4f}")
        
        # Test mAP with dummy data
        pred_boxes = [np.random.rand(5, 4) * 100 for _ in range(3)]
        pred_scores = [np.random.rand(5) for _ in range(3)]
        pred_labels = [np.random.randint(0, 3, 5) for _ in range(3)]
        gt_boxes = [np.random.rand(5, 4) * 100 for _ in range(3)]
        gt_labels = [np.random.randint(0, 3, 5) for _ in range(3)]
        
        map_results = compute_map(pred_boxes, pred_scores, pred_labels,
                                  gt_boxes, gt_labels, 3)
        print(f"  mAP@0.5 test result: {map_results['mAP@0_50']:.4f}")
        print(f"  ‚úì Metrics computation successful")
        results.append(True)
    except Exception as e:
        print(f"  ‚úó Metrics computation failed: {e}")
        results.append(False)
    print()
    
    return results


def test_evaluate_module():
    """Test evaluate.py module."""
    print("\n" + "=" * 80)
    print("2. TESTING EVALUATION MODULE (evaluate.py)")
    print("=" * 80 + "\n")
    
    results = []
    
    # Test imports
    results.append(test_import('evaluate', 'Import evaluate module'))
    print()
    
    # Test class
    results.append(test_class_exists('evaluate', 'BDD100kEvaluator', 'BDD100kEvaluator class'))
    print()
    
    # Test class initialization
    print("Testing: BDD100kEvaluator initialization")
    try:
        from evaluate import BDD100kEvaluator
        
        # Check if we can instantiate (without actually loading model)
        print("  ‚úì Class can be instantiated (would need model/data for full test)")
        results.append(True)
    except Exception as e:
        print(f"  ‚úó Failed: {e}")
        results.append(False)
    print()
    
    return results


def test_visualize_metrics_module():
    """Test visualize_metrics.py module."""
    print("\n" + "=" * 80)
    print("3. TESTING QUANTITATIVE VISUALIZATION MODULE (visualize_metrics.py)")
    print("=" * 80 + "\n")
    
    results = []
    
    # Test imports
    results.append(test_import('visualize_metrics', 'Import visualize_metrics module'))
    print()
    
    # Test class
    results.append(test_class_exists('visualize_metrics', 'MetricsVisualizer', 'MetricsVisualizer class'))
    print()
    
    return results


def test_visualize_predictions_module():
    """Test visualize_predictions.py module."""
    print("\n" + "=" * 80)
    print("4. TESTING QUALITATIVE VISUALIZATION MODULE (visualize_predictions.py)")
    print("=" * 80 + "\n")
    
    results = []
    
    # Test imports
    results.append(test_import('visualize_predictions', 'Import visualize_predictions module'))
    print()
    
    # Test class
    results.append(test_class_exists('visualize_predictions', 'QualitativeVisualizer', 'QualitativeVisualizer class'))
    print()
    
    return results


def test_failure_analysis_module():
    """Test failure_analysis.py module."""
    print("\n" + "=" * 80)
    print("5. TESTING FAILURE ANALYSIS MODULE (failure_analysis.py)")
    print("=" * 80 + "\n")
    
    results = []
    
    # Test file exists
    failure_analysis_path = script_dir / 'failure_analysis.py'
    if failure_analysis_path.exists():
        print(f"Testing: Failure analysis file exists")
        print(f"  Path: {failure_analysis_path}")
        print(f"  ‚úì File exists")
        results.append(True)
    else:
        print(f"Testing: Failure analysis file exists")
        print(f"  Path: {failure_analysis_path}")
        print(f"  ‚úó File not found")
        results.append(False)
    print()
    
    return results


def test_documentation():
    """Test documentation files."""
    print("\n" + "=" * 80)
    print("6. TESTING DOCUMENTATION")
    print("=" * 80 + "\n")
    
    results = []
    docs_dir = script_dir.parent / 'docs'
    
    # Check EVALUATION_REPORT.md
    eval_report = docs_dir / 'EVALUATION_REPORT.md'
    print(f"Testing: EVALUATION_REPORT.md exists")
    print(f"  Path: {eval_report}")
    if eval_report.exists():
        size = eval_report.stat().st_size
        print(f"  ‚úì File exists ({size:,} bytes)")
        results.append(True)
    else:
        print(f"  ‚úó File not found")
        results.append(False)
    print()
    
    # Check PHASE3_QUICKSTART.md
    quickstart = docs_dir / 'PHASE3_QUICKSTART.md'
    print(f"Testing: PHASE3_QUICKSTART.md exists")
    print(f"  Path: {quickstart}")
    if quickstart.exists():
        size = quickstart.stat().st_size
        print(f"  ‚úì File exists ({size:,} bytes)")
        results.append(True)
    else:
        print(f"  ‚úó File not found")
        results.append(False)
    print()
    
    # Check PHASE3_COMPLETE_SUMMARY.md
    summary = script_dir.parent / 'PHASE3_COMPLETE_SUMMARY.md'
    print(f"Testing: PHASE3_COMPLETE_SUMMARY.md exists")
    print(f"  Path: {summary}")
    if summary.exists():
        size = summary.stat().st_size
        print(f"  ‚úì File exists ({size:,} bytes)")
        results.append(True)
    else:
        print(f"  ‚úó File not found")
        results.append(False)
    print()
    
    return results


def test_dependencies():
    """Test required dependencies."""
    print("\n" + "=" * 80)
    print("7. TESTING DEPENDENCIES")
    print("=" * 80 + "\n")
    
    results = []
    
    dependencies = [
        ('numpy', 'NumPy'),
        ('cv2', 'OpenCV'),
        ('matplotlib', 'Matplotlib'),
        ('seaborn', 'Seaborn'),
        ('pandas', 'Pandas'),
        ('tqdm', 'tqdm'),
        ('ultralytics', 'Ultralytics YOLOv8'),
    ]
    
    for module_name, description in dependencies:
        print(f"Testing: {description}")
        print(f"  Module: {module_name}")
        try:
            importlib.import_module(module_name)
            print(f"  ‚úì Installed")
            results.append(True)
        except ImportError:
            print(f"  ‚úó Not installed")
            results.append(False)
        print()
    
    return results


def print_summary(all_results: dict):
    """Print validation summary."""
    print("\n" + "=" * 80)
    print("VALIDATION SUMMARY")
    print("=" * 80 + "\n")
    
    total_tests = 0
    passed_tests = 0
    
    for category, results in all_results.items():
        category_passed = sum(results)
        category_total = len(results)
        total_tests += category_total
        passed_tests += category_passed
        
        status = "‚úì PASS" if category_passed == category_total else "‚úó FAIL"
        print(f"{category:.<60} {category_passed}/{category_total} {status}")
    
    print()
    print("=" * 80)
    
    percentage = (passed_tests / total_tests * 100) if total_tests > 0 else 0
    
    if passed_tests == total_tests:
        print(f"üéâ ALL TESTS PASSED: {passed_tests}/{total_tests} ({percentage:.1f}%)".center(80))
        print("‚úÖ Phase 3 code is ready for evaluation!".center(80))
    else:
        failed = total_tests - passed_tests
        print(f"‚ö†Ô∏è  SOME TESTS FAILED: {passed_tests}/{total_tests} passed ({percentage:.1f}%)".center(80))
        print(f"{failed} test(s) failed - see details above".center(80))
    
    print("=" * 80)


def main():
    """Run all validation tests."""
    all_results = {}
    
    # Run all tests
    all_results['1. Metrics Module'] = test_metrics_module()
    all_results['2. Evaluation Module'] = test_evaluate_module()
    all_results['3. Quantitative Viz'] = test_visualize_metrics_module()
    all_results['4. Qualitative Viz'] = test_visualize_predictions_module()
    all_results['5. Failure Analysis'] = test_failure_analysis_module()
    all_results['6. Documentation'] = test_documentation()
    all_results['7. Dependencies'] = test_dependencies()
    
    # Print summary
    print_summary(all_results)
    
    # Next steps
    print("\nüìã NEXT STEPS:")
    print()
    print("1. Run Quick Evaluation Test (5 minutes):")
    print("   cd model/src/")
    print("   python evaluate.py --max-images 10")
    print()
    print("2. Generate Visualizations:")
    print("   python visualize_metrics.py")
    print("   python visualize_predictions.py --num-samples 5")
    print()
    print("3. View Results:")
    print("   open ../outputs/evaluation/charts/")
    print()
    print("4. Read Documentation:")
    print("   open ../docs/EVALUATION_REPORT.md")
    print("   open ../docs/PHASE3_QUICKSTART.md")
    print()


if __name__ == "__main__":
    main()
